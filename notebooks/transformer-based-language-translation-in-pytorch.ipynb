{
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30647,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# **English to French Translation with Transformers**\n",
    "\n",
    "In this notebook, you'll learn how to build your own ü§ñ English ‚Üí French translator using Transformers, the state-of-the-art model for natural language processing. We'll use the The Europarl parallel corpus is extracted from the proceedings of the European Parliament from 1996 to 2011[[EN->FR Dataset](https://www.statmt.org/europarl/)] and train the model with PyTorch Lightning‚ö°Ô∏è, a framework that makes training fast and easy.\n",
    "\n",
    "You'll also:\n",
    "\n",
    "- ‚öôÔ∏è Preprocess your text data into token tensors\n",
    "- üßë‚Äçüîß Design an encoder-decoder transformer architecture in PyTorch\n",
    "- üí¨ Translate English sentences into French\n",
    "\n",
    "Transformers are amazing models that use self-attention to capture the meaning and context of words. You'll see how they can help you create a powerful and elegant translator.\n",
    "\n",
    "\n",
    "üôÇ Let's get started!\n"
   ],
   "metadata": {
    "id": "2iembYmVERzi"
   }
  },
  {
   "cell_type": "markdown",
   "source": "# **üìä Data Exploration**\n\nIn this tutorial, we will explore the **Europarl** dataset, which is a widely used benchmark for evaluating the performance of machine translation systems. The dataset consists of the speeches delivered at the **European Parliament**, covering a variety of topics and domains. These speeches are available in **11 different languages**.\n\nThe creation of the dataset was lead by **Philipp Koehn**, a leading researcher and author in the field of machine translation.\n\nHere we will focus on just the **English to French** translation pair.",
   "metadata": {
    "id": "WjxkNBecMLd5"
   }
  },
  {
   "cell_type": "code",
   "source": "# Downloading the Dataset\nimport requests\nimport os\n\nurl = \"http://www.statmt.org/europarl/v7/it-en.tgz\"\nfilename = \"it-en.tgz\"\n\nif not os.path.exists(filename):\n    r = requests.get(url)\n    with open(filename, \"wb\") as w:\n        w.write(r.content)\n    print(\"File downloaded\")\nelse:\n    print(\"File already exists\")",
   "metadata": {
    "id": "ZQNf8d1qaYhW",
    "outputId": "c30698df-30a7-49a7-d33d-ecef307876c7",
    "execution": {
     "iopub.status.busy": "2024-08-24T15:52:08.592356Z",
     "iopub.execute_input": "2024-08-24T15:52:08.592720Z",
     "iopub.status.idle": "2024-08-24T15:52:20.589933Z",
     "shell.execute_reply.started": "2024-08-24T15:52:08.592691Z",
     "shell.execute_reply": "2024-08-24T15:52:20.588937Z"
    },
    "trusted": true
   },
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "text": "File downloaded\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "# Import the module for working with compressed tar files\nimport tarfile\n\n# Open a gzipped tar file in read mode\nwith tarfile.open(\"it-en.tgz\", \"r:gz\") as tar:\n    # Get a TarFile object for the archive\n    # Extract all the files and directories to the current folder\n    tar.extractall()",
   "metadata": {
    "id": "h9kuN9l-agv6",
    "execution": {
     "iopub.status.busy": "2024-08-24T15:52:20.591743Z",
     "iopub.execute_input": "2024-08-24T15:52:20.592043Z",
     "iopub.status.idle": "2024-08-24T15:52:25.300789Z",
     "shell.execute_reply.started": "2024-08-24T15:52:20.592017Z",
     "shell.execute_reply": "2024-08-24T15:52:25.299859Z"
    },
    "trusted": true
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "#### **Preprocessing**\nWe will perform text cleaning for Neural Machine Translation (NMT) Model.\nThis is useful because it can:\n\n- Remove irrelevant or noisy information, such as HTML tags, punctuation, capitalization, or non-printable characters, that might confuse the NMT model or reduce its performance.\n\n- Normalize the text to a consistent format, such as lowercase letters, UTF-8 encoding e.t.c. That can be easily processed by the NMT model.\n\nAnd many other benefits....\n\n> By cleaning the text, we can make it more suitable for NMT and improve the quality and accuracy of the translation output. üòä",
   "metadata": {
    "id": "yrO_t-tEbij8"
   }
  },
  {
   "cell_type": "code",
   "source": "import re\n\n# Define a function for text cleaning\ndef clean_text(text):\n\n    # Convert text to lowercase\n    text = str(text).lower().strip()\n\n    # Remove the \\n at the end of each line in the file\n    text = text.rstrip('\\n')\n\n    # Remove HTML tags and non-alphanumeric characters\n    text = re.sub(r\"<[^>]+>\", \"\", text)\n    text = re.sub(r\"[^a-zA-Z√Ä-√ø0-9\\s.,;!?':()\\[\\]{}-]\", \" \", text)  # Keep selected punctuation marks, symbols and apostrophes\n\n    # Remove excessive whitespace (more than one space)\n    text = re.sub(r\"\\s+\", \" \", text)\n\n    text = text.encode(\"utf-8\", errors=\"ignore\").decode(\"utf-8\")  # Corrected encoding\n\n    return text",
   "metadata": {
    "id": "N3h8-eN9bMwp",
    "execution": {
     "iopub.status.busy": "2024-08-24T15:52:25.301846Z",
     "iopub.execute_input": "2024-08-24T15:52:25.302145Z",
     "iopub.status.idle": "2024-08-24T15:52:25.308551Z",
     "shell.execute_reply.started": "2024-08-24T15:52:25.302120Z",
     "shell.execute_reply": "2024-08-24T15:52:25.307514Z"
    },
    "trusted": true
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "import pandas as pd\n\n# Read from the two text files, and create a pandas Series object (like a python list[]), and clean the text by\n# applying our clean_text function to every element.\n\nwith open('europarl-v7.it-en.en', 'r') as en_file, open('europarl-v7.it-en.it', 'r') as fr_file:\n    en = pd.Series(en_file.readlines(), name='en').apply(lambda text: clean_text(text))\n    fr = pd.Series(fr_file.readlines(), name='it').apply(lambda text: clean_text(text))",
   "metadata": {
    "id": "A5zBTPoidjeX",
    "execution": {
     "iopub.status.busy": "2024-08-24T15:52:25.310851Z",
     "iopub.execute_input": "2024-08-24T15:52:25.311121Z",
     "iopub.status.idle": "2024-08-24T15:53:53.128046Z",
     "shell.execute_reply.started": "2024-08-24T15:52:25.311098Z",
     "shell.execute_reply": "2024-08-24T15:53:53.127077Z"
    },
    "trusted": true
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Let's merge the two series objects into a Pandas Dataframe, which will create two columns\n# 'en' and 'fr' corresponding to the sentence pairs.\n\ntranslation_df = pd.concat([en, it], axis=1)\n\n# Show 5 sample rows of the dataframe.\ntranslation_df.head()",
   "metadata": {
    "id": "x3WiDCRbf7an",
    "outputId": "381d6068-23a9-44e0-e8be-ce6154f6eb00",
    "execution": {
     "iopub.status.busy": "2024-08-24T15:53:53.129189Z",
     "iopub.execute_input": "2024-08-24T15:53:53.129479Z",
     "iopub.status.idle": "2024-08-24T15:53:53.163413Z",
     "shell.execute_reply.started": "2024-08-24T15:53:53.129453Z",
     "shell.execute_reply": "2024-08-24T15:53:53.162263Z"
    },
    "trusted": true
   },
   "execution_count": 9,
   "outputs": [
    {
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Let's merge the two series objects into a Pandas Dataframe, which will create two columns\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# 'en' and 'fr' corresponding to the sentence pairs.\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m translation_df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mconcat([en, \u001B[43mit\u001B[49m], axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# Show 5 sample rows of the dataframe.\u001B[39;00m\n\u001B[1;32m      7\u001B[0m translation_df\u001B[38;5;241m.\u001B[39mhead()\n",
      "\u001B[0;31mNameError\u001B[0m: name 'it' is not defined"
     ],
     "ename": "NameError",
     "evalue": "name 'it' is not defined",
     "output_type": "error"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "# There are just above 2 Million sentence pairs.\nlen(translation_df)",
   "metadata": {
    "id": "sbZL-KKlgjWe",
    "outputId": "f19ab989-67bf-4189-d851-f12c76ad9de0",
    "execution": {
     "iopub.status.busy": "2024-08-24T15:53:53.165596Z",
     "iopub.status.idle": "2024-08-24T15:53:53.165909Z",
     "shell.execute_reply.started": "2024-08-24T15:53:53.165753Z",
     "shell.execute_reply": "2024-08-24T15:53:53.165766Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "!pip install sqlalchemy",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-08-24T15:53:53.164196Z",
     "iopub.status.idle": "2024-08-24T15:53:53.164521Z",
     "shell.execute_reply.started": "2024-08-24T15:53:53.164351Z",
     "shell.execute_reply": "2024-08-24T15:53:53.164364Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# We will Create an SQLite Database for the dataframe for faster access to specific lines.\nfrom sqlalchemy import create_engine\n\nDATABASE_NAME = 'translation.db'\nTABLE_NAME = 'en_it'\n\n# Create a SQLite database\nengine = create_engine(f'sqlite:///{DATABASE_NAME}')\n\n# store the pandas dataframe in the created Sqlite db.\ntranslation_df.to_sql(TABLE_NAME, engine, if_exists='append')\n\nprint(f'Moved to sqlite db')",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-09T11:11:50.122534Z",
     "iopub.status.busy": "2024-02-09T11:11:50.122192Z",
     "iopub.status.idle": "2024-02-09T11:12:15.407384Z",
     "shell.execute_reply": "2024-02-09T11:12:15.406168Z",
     "shell.execute_reply.started": "2024-02-09T11:11:50.122506Z"
    },
    "id": "2AXINywIgy8y",
    "outputId": "1e3c4730-4c7e-42b1-faa2-ccac48d35b8a"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "TRANSLATION_DB_FILE_PATH = 'translation.db'",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-09T11:15:31.251317Z",
     "iopub.status.busy": "2024-02-09T11:15:31.250784Z",
     "iopub.status.idle": "2024-02-09T11:15:31.255613Z",
     "shell.execute_reply": "2024-02-09T11:15:31.254845Z",
     "shell.execute_reply.started": "2024-02-09T11:15:31.251285Z"
    },
    "id": "kJXEkgzfh2rS"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# **Text Tokenization**\n\nText tokenization is a key step in natural language processing (NLP) that enables computers to understand human language. It converts text into numerical representations that are easier for machines to interpret.\n\n<br>\n\n### **Why Tokenize Text ‚ùì**\n\nHuman communication has structure through words, sentences, grammar, and so on. Tokenization exposes this structure so that machines can discover patterns, relationships, and meaning. It splits text into smaller units called tokens that can be assigned numerical values.\n\nCommon types of tokens are:\n\n- **Words**: Words are the basic units of meaning in language, such as \"love\", \"Paris\", or \"the\".\n\n- **Subwords**: Subwords are parts of words that have some meaning or function, such as prefixes, suffixes, or stems.\n\nWithout tokenization, text is just a sequence of characters without shape or meaning. Tokens add form that machines can process.\n\n<br>\n\n### **Challenges in Tokenization üöß**\n\nTokenizing human text is challenging because languages have complex and diverse rules:\n\n- Word spaces vary across languages. Some languages, such as Chinese and Japanese, do not use whitespace to separate words.\n\n- Made-up words, such as names, slang, or acronyms, may not be recognized or split correctly by tokenizers.\n\n- The same word can have different meanings depending on the context. Capturing the nuances of language is difficult for machines.\n\nGood tokenizers handle these challenges with techniques such as vocabulary lists, multi-word tokens, word splitting rules, and more.\n\n<br>\n\n### **Assigning Meaning Through Embeddings üìà**\n\nTokenization structures text, embeddings provide meaning. They map the tokens to numerical vectors that capture its information in the language. Similar tokens have similar embeddings (e.g eat & drink, walk & run, cat & dog e.t.c)\n\n**Note**: We will come back to this later...\n",
   "metadata": {
    "id": "WYI7IzGMRsg1"
   }
  },
  {
   "cell_type": "markdown",
   "source": "# Using a Tokenizer üõ†Ô∏è\n\nNow that you know what text tokenization is and why it is important, let's see how we can do it in practice. We will use a pretrained multi-lingual tokenizer from the **Hugging Face Tokenizers Library** for both English and French texts.\n\n**Here are some of the steps performed by the tokenizer:**\n\n1. The tokenizer uses **WordPiece**, a way to split words into smaller parts that make sense. For example, the word \"Transformer\" can be split into \"Trans\" and \"former\". This helps the computer to learn new words and save space.\n\n    WordPiece works by finding the most common parts of words in a text and combining them together. For example, if the text has many words that end with \"ing\", WordPiece will merge \"i\", \"n\", and \"g\" into one part. **This way, having seen \"eat\" and \"ing\", the model can infer the meaning of the word \"eating\".**\n\n2. It also **lowercases** the text, and splits on **whitespace** and **punctuation**. For example, the sentence \"Hello world!\" can be split into \"hello\", \"world\", and \"!\".\n\n3. It uses **special tokens** to mark the distinctions in the sentence, and to handle unknown or padding tokens. I am going to be using the [CLS] and [SEP] tokens as the beginning and ending of a sentence. For example, the sentence \"I love Paris\" will be tokenized as \"[CLS]\", \"i\", \"love\", \"paris\", \"[SEP]\".\n\nSo the special tokens are:\n- \"[CLS]\" for start of sentence\n- \"[SEP]\" for end of sentence\n- \"[UNK]\" for unknown token\n- \"[PAD]\" for padding token\n\n<br>\nüìù Technical Detail: [CLS] and [SEP] are meant as classification and separation tokens in the pretrained tokenizer, but since it does not have [SOS] and [EOS] tokens (which it shouldn't being a BERT tokenizer), I am swapping it's use.",
   "metadata": {
    "id": "icex3fQ0ihdt"
   }
  },
  {
   "cell_type": "code",
   "source": "# Install the necessary library\n!pip install transformers",
   "metadata": {
    "id": "cJ8QUCGVmxj3"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Download the pretrained tokenizer\nfrom transformers import AutoTokenizer\n\n# Save the name of the model whose tokenizer we are using. We will need it later.\nPRE_TRAINED_MODEL_NAME = \"distilbert/distilbert-base-multilingual-cased\"\n\n# Download the tokenizer\ntokenizer = AutoTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-09T11:15:42.501608Z",
     "iopub.status.busy": "2024-02-09T11:15:42.501233Z",
     "iopub.status.idle": "2024-02-09T11:16:06.785052Z",
     "shell.execute_reply": "2024-02-09T11:16:06.784252Z",
     "shell.execute_reply.started": "2024-02-09T11:15:42.501578Z"
    },
    "id": "VbksStKCnBq1"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Get the vocabulary size of our tokenizer (Number of unique words, subwords e.t.c. That the tokenizer understand.)\n# Note: Anything any text not in its vocabulary is replace with the [UNK] special tokens.\n# Note: The Special tokens are also included in the size of the vocabulary\ntokenizer.vocab_size",
   "metadata": {
    "id": "T0u1H6gJoPJc",
    "outputId": "80126870-a88d-4b0d-b9b3-cd81ffaf5420"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Special tokens available in the tokenizer.\ntokenizer.all_special_tokens",
   "metadata": {
    "id": "ZccoDFKVo99p",
    "outputId": "c82f7b83-0dfb-4ba0-ba9e-4ac9bc0e9a18"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# View of 5 tokens and their number (called token id) in our vocabulary\nprint (list(tokenizer.get_vocab().items()) [21:26])",
   "metadata": {
    "id": "1qC4M6ey654j",
    "outputId": "d60db7ac-6691-4f7e-d54b-5b091c07d8a7"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Let's test our tokenizer out on both english and french text.\ntokenizer(['fran√ßais bonjour', 'morning francais']).input_ids",
   "metadata": {
    "id": "tJfOwRTXoLg7",
    "outputId": "4c7d5eb9-5f07-4d5d-ccbd-fd70ae25059d"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Above you see two lists where the first and last number is 101, and 102 respectively.\n# This corresponds to our [CLS] and [SEP] token which we will be using at the beginning and\n# ending of a sentence.\nprint('[CLS] : ', tokenizer.cls_token_id)\nprint('[SEP] : ', tokenizer.sep_token_id)\n\n# But what do this numbers mean ??. Well let's continue our journey üõ£Ô∏è.",
   "metadata": {
    "id": "tAtiVjZxpbEQ",
    "outputId": "43eeed2b-9c81-4e55-aae1-e2ccc12a1a63"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Word Embeddings: A Simple Explanation üìö\n\nWord embeddings are vectors of real numbers ( e.g [ 0.1, 0.4, -0.8] ), one per token in your vocabulary. They are used to represent the semantic meaning of words in a way that is efficient and comparable.\n\nTo create this word embeddings, we had to tokenize the text, that is, convert the words into numbers. This assigns a unique integer to each token in the vocabulary. For example, the sentence ‚Äúmorning francais‚Äù was tokenized as [28757, 63184, 12985] (Note: This was the tokenization before we added our [CLS] and [SEP]).\n\nBut these numbers don‚Äôt tell us much about the words. They don‚Äôt tell us what the words mean, or how they are related to each other. For example, we don‚Äôt know if dog and cat are similar or different, or if apple and orange are fruits or colors.\n\nThat‚Äôs why we need to use an embedding layer, that maps this numbers to their corresponding vectors.\n    \n    For example:\n\n    dog -- (tokenized to 1) -- mapped to row 1 of matrix: [0.2, -0.1, 0.5, ‚Ä¶]\n    cat -- (tokenized to 23) -- mapped to row 23 of matrix:  [0.3, -0.2, 0.4, ‚Ä¶]\n    apple -- (tokenized to 40) -- mapped to row 40 of matrix:  [-0.1, 0.4, 0.2, ‚Ä¶]\n    orange -- (tokenized to 22) -- mapped to row 22 of matrix:  [-0.2, 0.3, 0.1, ‚Ä¶]\n    ...\n\nThis vectors called embeddings, are like arrows that point in different directions and have different lengths (size of the arrow not the number of elements e.g The arrow -- [0.5, 0.7] is longer than the arrow -- [0.2, 0.1] ).\n\nThis embeddings are stored in an embedding matrix and when our neural network model is being trained it tries to make the vectors match the meaning and context of the words. For example, it tries to make the vectors of similar words point in the same direction, and the vectors of different words point in different directions. It also tries to make the vectors of words that are often used together have similar lengths, and the vectors of words that are rarely used together have different lengths.\n\nThis will enable it to understand the contextual meaning of text better, but this requires training it with millions or even billions of books, texts e.t.c.\n\nThat's why it is more common to use embedding layers of models that have already been trained.\n\nNote: As you saw embedding maps the token numbers from the tokenizer to rows in the embedding matrix. This means the embedding matrix will be of size (num_rows, num_cols) -> (tokenizer_vocab_size, embed_dim) where embed_dim is the number of elements in the vector of embedding used to represent a token. Common sizes are 256, 300, 512, 786 e.t.c.\n\nNote: This also implies that the tokenizer and embedding should come from the same pre-trained model for the mapping token numbers -> Row, to match.",
   "metadata": {
    "id": "2x4xNAHSqtrz"
   }
  },
  {
   "cell_type": "code",
   "source": "# Get the embedding layer from our pre-trained model.\n\nfrom transformers import AutoModelForMaskedLM\n\n# Note üëÄ how we are using the same model name.\npre_trained_model = AutoModelForMaskedLM.from_pretrained(PRE_TRAINED_MODEL_NAME) # downloads the model.\n\n# Fetch the embedding layer from the pre-trained model.\nembedding_layer = pre_trained_model.get_input_embeddings()\n\n# These line just tells pytorch we don't intend to further train the embedding layer.\n# So it freezes the layers knowledge, so we don't scatter it while our model is still starting to learn.\nembedding_layer = embedding_layer.requires_grad_(False)",
   "metadata": {
    "id": "-j2IV3y7qs05",
    "outputId": "8df17716-2f86-4855-c880-8697ccfbd8f8"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "print('Vocabulary Size :', tokenizer.vocab_size)\nprint('Embedding Layer :', embedding_layer)\n\n# As you can see the number of rows of the embedding_layer match up with the vocab size.\n# We can also see that the embed_dim size (num_of elements in embedding vector) used here is 768.",
   "metadata": {
    "id": "LYy5l573n9NZ",
    "outputId": "7c268933-c776-4cdb-cfc5-12d297e91286"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Preparing Our Translation Dataset",
   "metadata": {
    "id": "AhnUoC1y6rL5"
   }
  },
  {
   "cell_type": "code",
   "source": "# Install pytorch lightning\n!pip install lightning",
   "metadata": {
    "id": "D_1_JCg686Wz"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Pytorch\nimport torch\n# To access our sqlite db\nimport sqlite3\n\n# other tools needed\nfrom torch.utils.data import *\nimport torch.nn as nn",
   "metadata": {
    "id": "kgzG_C2B9Th8"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Understanding the form of Our Dataset.\n\nThe transformer we are going to use (which we will see later) is going to have an encoder and decoder layer.\n\n<br>\n\n#### Encoder Overview üîÄ\n\nThe üïµÔ∏è‚Äç‚ôÇÔ∏è encoder creates meaningful representations of its inputs (in this case, an English sentence) üí¨.\n\n<br>\n\n#### Decoder Overview üì°\n\nThe üì° decoder then uses these representations to perform a specific task (translating to French üá´üá∑ in this example).\n\n<br>\n\n#### Translation Approach üéØ\n\nWe will implement translation using a common technique called **next token prediction** ‚è≠Ô∏è.\n\nAt each timestep, the decoder uses the encoder's output to predict the next word in the translated sequence.  \n\n#### Example üí°\n\nFor example, to translate \"Beautiful day\" into \"Belle journ√©e\", we first append special start `[CLS]` and end `[SEP]` tokens to the French translation:\n\n> \"[CLS] Belle journ√©e [SEP]\"\n\nOur goal is to train the model to predict the next token at each step:\n\n1. When it sees `[CLS]`, predict \"Belle\"\n2. When it sees \"Belle\", predict \"journ√©e\"\n3. When it sees \"journ√©e\", predict `[SEP]` ‚úÖ  \n\n<br>\n\n#### Teacher Forcing üë©‚Äçüè´  \n\nTraining the model with the above üëÜüëÜ method will be very slooow.\n\nSo instead, we use a technique called **teacher forcing** üë©‚Äçüè´.\n\nWe provide the full ground truth translation up to the `[SEP]` token to the decoder during training. Just \"`[CLS]` Belle journ√©e\".\n\nThen we set the models target outputs to be \"Belle journ√©e `[SEP]`\".\n\n<br>\n\n#### Causal Masking üò∑  \n\nSince we are passing in the french translation to the decoder layer in training, we employ something called **causal masking** üò∑ (more on this later) in the decoder to prevent it from cheating by looking at the full output translation.\n\nThis forces the model to predict the next token based only on the encoder outputs and what came before in the decoder outputs.  \n<br>\n\n#### Inference Process ü§î\n\nAt inference time, we pass an English input to the encoder and just the `[CLS]` token to the decoder initially.\n\nWe then feed the models predicted token from the previous timestep back into the decoder to predict the next token.\n\nWe continue this loop until the `[SEP]` token is predicted, indicating the ‚úÖ end of translation.   \n<br>\n\n#### Note üìù\nIn most sequence-to-sequence models (like ours), the encoder inputs do not contain start/end tokens - those are appended to the decoder inputs only.\n\nThis allows the decoder to know when to start üèÅ and stop ‚úã generating the translation.\n\n<br>\n\n#### **Batching**\nBatching allows us to make more efficient use of computing power by training on batches of sentence pairs rather than one sentence pair at a time. Processing batches enables the model to learn general linguistic patterns across sentences, rather than potentially noisy patterns within individual sentences.\n\nHowever, sentences come in varying lengths. To create same sized batches, shorter sentences will be padded with special `[PAD]` tokens to match the length of the longest sentence in the batch.\n\nWe will configured the model to ignore these pad tokens. Along with the batch of sentences, the actual lengths of each underlying sentence will also be passed to the model so it knows how much of each sentence contains real words versus padding.\n\nThis contextual information on real sentence lengths allows the model to differentiate between content words and pads inserted to standardize batch lengths.\n\n<br>\n\n\n#### **Using this information let's create the dataset for our model**",
   "metadata": {
    "id": "foH9l_6R-d13"
   }
  },
  {
   "cell_type": "code",
   "source": "# Define some constants for our model architecture\nTOKEN_LIMIT = 350  # The maximum number of tokens our model can handle in a sentence\nPAD_IDX = tokenizer.pad_token_id  # The pad token id to use in padding shorter sentences in the batch\n\n# This class inherits from a pytorch dataset, and its function is to load and determine how to get a particular sample\n# from the data using the __getitem__ function. The collate_fn function's job is to get many samples and batch them together.\nclass EN_FR_Dataset(Dataset):\n\n    TOTAL_SAMPLES = 2_007_723  # Number of sentence pairs in our dataset\n\n    def __init__(self, *, db_path, tokenizer):\n        # Create a connection to the database\n        self.conn = sqlite3.connect(db_path)\n        self.cursor = self.conn.cursor()\n\n        self.tokenizer = tokenizer\n\n    def __getitem__(self, index):\n        # Execute a query to fetch the English and French sentences at the given index\n        self.cursor.execute('SELECT en, fr FROM en_fr WHERE \"index\" = ?', (index,))\n        row = self.cursor.fetchone()\n\n        # Raise an exception if the row is not found\n        if row is None:\n            raise Exception('Row not found at index', index)\n\n        try:\n            # Encode the English and French sentences using the tokenizer, adding the special start [CLS] and end sentence [SEP]\n            # tokens to only our french translation which will both in the input to the decoder layer and target output of the model.\n            en_encoded = self.tokenizer.encode(row[0], add_special_tokens=False)\n            fr_encoded = self.tokenizer.encode(row[1], add_special_tokens=True)\n\n            # If the sentence length is greater than the token limit, replace it with a predefined sample\n            if len(en_encoded) > TOKEN_LIMIT or len(fr_encoded) > TOKEN_LIMIT - 2: # Note: The -2 is because we added [CLS] and [SEP] to it.\n                return self.__getitem__(0)\n\n            # Convert the encoded sentences to PyTorch tensors\n            en_tensor = torch.tensor(en_encoded)\n            fr_decoder_input = torch.tensor(fr_encoded[:-1])  # Input for decoder (excluding end token)\n            fr_decoder_label = torch.tensor(fr_encoded[1:])  # Label for decoder (excluding start token)\n\n        except Exception:\n            # If any error occurs, return the first sample as a fallback\n            return self.__getitem__(0)\n\n        # Return the tensors along with the acutual length of the english sentence.\n        return en_tensor, len(en_encoded), fr_decoder_input, fr_decoder_label\n\n    def __len__(self):\n        # Return the total number of samples in the dataset\n        return EN_FR_Dataset.TOTAL_SAMPLES\n\n    def close(self):\n        # Close the database connection\n        self.conn.close()\n\n    @staticmethod\n    def collate_fn(batch):\n        # `batch` is a list of samples of the form [(en_tensor, len(en_encoded), fr_decoder_input, fr_decoder_label), ...]\n        # so we unpack every tuple in the list.\n        en_sentences, en_seq_lengths, fr_decoder_inputs, fr_decoder_labels = zip(*batch)\n\n        # Pad every sequence in the batch to the max sequence length using pad_sequence with PAD_IDX for padding\n        padded_en_sentences = nn.utils.rnn.pad_sequence(en_sentences, batch_first=True, padding_value=PAD_IDX)\n        padded_fr_decoder_inputs = nn.utils.rnn.pad_sequence(fr_decoder_inputs, batch_first=True, padding_value=PAD_IDX)\n        padded_fr_decoder_labels = nn.utils.rnn.pad_sequence(fr_decoder_labels, batch_first=True, padding_value=PAD_IDX)\n\n        # Convert sequence lengths to a LongTensor (Pytorch requires it this way).\n        en_seq_lengths = torch.as_tensor(en_seq_lengths, dtype=torch.long)\n\n        # Return the padded tensors and sequence lengths\n        return padded_en_sentences, en_seq_lengths, padded_fr_decoder_inputs, padded_fr_decoder_labels\n",
   "metadata": {
    "id": "VOyFZwcw8w3z"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Import LightningDataModule from PyTorch Lightning\nimport lightning as L\n\n# Define a custom class that inherits from LightningDataModule\n# A LightningDataModule organizes data-related code\n# It separates data processing from model training\n# It also makes data code reusable and shareable\nclass TranslationDataModule(L.LightningDataModule):\n\n    # Define the constructor with three arguments\n    # batch_size: number of samples per iteration\n    # num_workers: number of processes for data loading\n    # train_ds: dataset object with translation pairs\n    def __init__(self, batch_size, num_workers, train_ds):\n        super().__init__()\n\n        # Assign the arguments to attributes\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n        self.train_ds = train_ds\n\n    # Override the setup method\n    # This method is called before data loaders are created\n    # It is used for data preprocessing or splitting\n    def setup(self, stage: str):\n        # Split train_ds into training and validation datasets with an 80/20 ratio\n        self.train_ds, self.val_ds = random_split(\n          self.train_ds,\n          (0.8, 0.2)\n        )\n\n    # Define the dataloader methods for both training and validation.\n    # It returns a DataLoader object for training data\n    # A DataLoader handles batching, shuffling, and sampling of the data gotten from our dataset.\n    # It also supports multiprocessing and prefetching\n    def train_dataloader(self):\n        # Create and return a DataLoader with these arguments:\n        # - dataset: train_ds with training data\n        # - batch_size: batch_size attribute\n        # - num_workers: num_workers attribute\n        # - shuffle: True to shuffle the training data before each epoch (An epoch is a full iteration through our entire dataset).\n        # - prefetch_factor: 2 to prefetch 2 samples per worker\n        # - collate_fn: our function to combine samples into a batch\n        return DataLoader(\n          self.train_ds,\n          batch_size=self.batch_size,\n          num_workers=self.num_workers,\n          shuffle=True,\n          prefetch_factor=2,\n          collate_fn=EN_FR_Dataset.collate_fn\n          )\n\n    # Define the val_dataloader method\n    # It returns a DataLoader object for validation data\n    # It is similar to train_dataloader, but uses val_ds as dataset\n    def val_dataloader(self):\n        # Note: We don't shuffle the validation dataset. We use this dataset to evaluate the model's\n        # performance after each epoch.\n        return DataLoader(\n          self.val_ds,\n          batch_size=self.batch_size,\n          num_workers=self.num_workers,\n          prefetch_factor=2,\n          collate_fn=EN_FR_Dataset.collate_fn\n          )",
   "metadata": {
    "id": "LELO_S1Ajtcb"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# create the dataset by pointing to the sqlite db file we created earlier.\ntranslation_dataset = EN_FR_Dataset(db_path=TRANSLATION_DB_FILE_PATH, tokenizer=tokenizer)",
   "metadata": {
    "id": "2Rkh1A_8onUl"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# test our dataset by attempting to get the first sample\ntranslation_dataset[0]",
   "metadata": {
    "id": "m9NXMyD_pY_0",
    "outputId": "ebff63b3-6561-4248-dcf3-8a124f90ca48"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# create our translation data module that handles loading batches of data from our dataset.\ntranslation_datamodule = TranslationDataModule(\n    batch_size=24,\n    num_workers=2,\n    train_ds=translation_dataset)\n\n# Note: Typically larger batch sizes are used e.g 128 even up to 1024, as much as possible to fully utilize your gpu.\n# But as you will see the model is large and my gpu ram was small (just 15 GB), so i had to go with batch size 24.",
   "metadata": {
    "id": "z_BRTW72pqTF"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# **Transformer Architecture**\nPhew! üòå We've done the hard work of creating a dataset to train our model. Now it's time to enjoy the fruits of our labor - building a transformer model.\n\nWe will be building a popular variant of the orginial transformer  called the ReZero Transformer. It's a neat variant that's simpler, faster, and more stable.\n\nHow cool is that? üòé Let's continue our journey üõ£Ô∏è\n",
   "metadata": {
    "id": "OR7hROuGr7yg"
   }
  },
  {
   "cell_type": "markdown",
   "source": "<img src=\"https://onedrive.live.com/embed?resid=8C3CCBBA832CF1E0%21601&authkey=%21AHHEOlJE806ebXk&width=724&height=832\" width=\"724\" height=\"832\" />",
   "metadata": {
    "id": "JNbBPQHr30as"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The ReZero transformer is a simple modification of the standard transformer architecture that improves signal propagation and convergence speed. It replaces the layer normalization (the Norm in Add & Norm in the figure above üëÜ) with a learned residual skip connection.\n",
    "\n",
    "This means that the layer starts with the same data as the previous layer and only adds a small amount of new information to it. This small amount is learned by the layer and can be changed as needed.\n",
    "\n",
    "‚≠ê Picture credits: [Borealis AI](https://www.borealisai.com/research-blogs/tutorial-17-transformers-iii-training/#Better_methods_for_training_transformers)"
   ],
   "metadata": {
    "id": "wGNdAWml8QBq"
   }
  },
  {
   "cell_type": "markdown",
   "source": "<img src=\"https://onedrive.live.com/embed?resid=8C3CCBBA832CF1E0%21598&authkey=%21ANA81xeriJk1m5o&width=2560&height=641\" width=\"800\" height=\"201\" />",
   "metadata": {
    "id": "U48cBbUK7zNG"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## **Explanation & Implementation**\nIn this section, we will explain and implement our transformer step by step. We have already learned about the **embedding layer**, which maps `token ids` to `vectors`.\n\nThis layer is shown as the **input and output embeddings** in the transformer picture above üëÜüëÜ.\n\nSo, we will start our explanation from **positional encoding**.\n",
   "metadata": {
    "id": "DBxSpAXKDAxI"
   }
  },
  {
   "cell_type": "markdown",
   "source": "### **Positional Encodings**\nThe position of each word in a sentence affects its meaning. For example, \"I love pizza\" üçï and \"Pizza love I\" have the same words, but different orders and meanings.\n\nWe can use positional encoding to represent the position of each word as a vector, similar to `word embeddings`. This way, we can teach a computer to understand the order and the context of the words in a sentence.\n\nThere are two ways of creating positional encoding vectors:\n\n- Fixed positional encoding: The vectors are predefined and fixed. Formulas such as sine and cosine functions is used:\n\n$$PE_{(pos,2i)} = sin(pos / 10000^{2i / d_{model}})$$\n$$PE_{(pos,2i+1)} = cos(pos / 10000^{2i / d_{model}})$$\n\nwhere $pos$ is the position, $i$ is the dimension and $d_{model}$ is the size of embedding dimension (number of elements in positional encoding vector). This way captures relative distances and handle variable length sentences. *`[This is what the original transformer paper did]`*.\n\n- Learned positional encoding: The vectors are randomly initialized and learned by the model. An embedding layer is used with different embeddings for each position. This way the model learns the positional information to add to the data enabling it to potentially capture more complex patterns. *`[This is what most state-of-the-art models do]`*.\n\nNote: When we are using learned positional encodings we are going to need to put a limit on the number of tokens our model can take in. This is because the position encoding layer is going to have an embedding matrix with as many rows as our max number of tokens. Its number of columns should also match those of the word embeddings because we are going to add `+` them together, this way we get a new vector that contains both the meaning and the position of each word.\n\nIn this tutorial, we use the positional encoding layer from the `pre_trained_model`. It has a token limit of 512, and our model `TOKEN_LIMIT=350` so we are good to go.\n",
   "metadata": {
    "id": "pGl7GaaYDXgQ"
   }
  },
  {
   "cell_type": "code",
   "source": "pos_embedding_layer = pre_trained_model.get_position_embeddings()\npos_embedding_layer = pos_embedding_layer.requires_grad_(False) # Freezes knowledge as we have seen before.\n\npos_embedding_layer # Note how it supports up to 512 tokens and has an embed dim of 768 just like our word embedding layer.",
   "metadata": {
    "id": "dpk7utlUsVcm",
    "outputId": "2f45b3a6-3bfc-4edc-a82e-3e9bc7345366"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "class LearnedPositionalEncoding(nn.Module):\n\n    def __init__(self, *, pos_embed_layer):\n        \"\"\"\n        Initializes the module with a learned positional embedding layer.\n\n        Args:\n            pos_embed_layer: Our Pre-Trained nn.Embedding layer containing positional encodings.\n        \"\"\"\n        super().__init__()\n        self.positional_embedding = pos_embed_layer\n\n    def forward(self, X):\n        \"\"\"\n        Adds learned positional encodings to the input sequence.\n\n        Args:\n            X: Input sequence of token embeddings (batch_size, sequence_length, embedding_dim).\n\n        Returns:\n            Encoded sequence with positional information added (same shape as X).\n        \"\"\"\n\n        position_indices = torch.arange(X.size(1), device=X.device)  # Get position indices\n        positional_embeddings = self.positional_embedding(position_indices)  # Lookup position embeddings\n\n        # Expand positional embeddings for batch-wise operation\n        expanded_positional_embeddings = positional_embeddings.unsqueeze(0)\n\n        return X + expanded_positional_embeddings  # Add embeddings to input sequence\n",
   "metadata": {
    "id": "0Y4gJDQhsMbZ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### **Multi-Head Attention & Masked-Head Attention**",
   "metadata": {
    "id": "kICwzQz1udGC"
   }
  },
  {
   "cell_type": "markdown",
   "source": "<img src=\"https://onedrive.live.com/embed?resid=8C3CCBBA832CF1E0%21597&authkey=%21ABpALYIXmMaQ8os&width=1834&height=842\" width=\"700\" height=\"300\" />",
   "metadata": {
    "id": "-3cWjgppusDM"
   }
  },
  {
   "cell_type": "markdown",
   "source": "**Before we understand Multi-Head & Masked Multi Head Attention we need to understand Scaled Dot-Product Attention**\n\n#### Scaled Dot-Product Attention (SDPA)\n\nImagine a vibrant party teeming with words, each eager to understand its peers. This is the essence of Scaled Dot-Product Attention (SDPA), the heart of the Transformer, a powerful language model. But before we hit the dance floor, let's meet the key players:\n\n**Word Embeddings:** Think of these as name tags at the party, each word getting a unique vector representing its meaning. But in sentences like \"Do what is Right\" and \"Shift to your Right,\" the word \"Right\" has the same tag despite differing contexts.\n\n**Enter SDPA, the party game changer!** It introduces three roles:\n\n- **Query (Q):** The curious word asking, \"Who has the information I need?\"\n- **Key (K):** Like a name tag, revealing relevant skills or knowledge.\n- **Value (V):** The actual hidden talent or information the word possesses.\n\nNow, picture each word comparing its query (e.g., \"Who has the information I need?\") with everyone's keys (e.g The information they have). The more relevant the key's information, the higher the \"attention score\" it gets. Think of it as noticing someone with a matching talent you need!\n\nBut how does everyone stay informed about these scores? This is where the **`attention weight matrix`** comes in! It's like a giant scoreboard displayed at the party, where each cell shows the attention score between a specific word pair. For example, the cell at row \"Right\" and column \"Shift\" would hold the score indicating how well these words relate.\n\nBut how does this translate into updating the word embeddings?\nThat's where **softmax** steps in, acting as the **regulator** who assigns weights to each score based on its relative importance.\n\nImagine the **regulator** listening to each word's scores and saying, \"Okay, so 'Shift' is quite relevant for determining 'Right' meaning in this context, so you get a high weight.\"\n\nHere's how softmax works its magic:\n\n1. **Listen to All Scores:** It takes all the attention scores for a specific word (a row in the our `attention weight matrix`) as input.\n\n2. **Apply the Formula:** It uses a mathematical formula to consider the relative strength of each score compared to the others. This ensures that scores that are much higher than others have a larger impact on the final weights.\n\n3. **Distribute the Weights:** Softmax transforms these relative strengths into weights between 0 and 1, ensuring everyone gets a fair share of attention but the most relevant ones get a bigger slice.\n\nNow, each word has a set of weighted values based on its interactions with others. Think of it as collecting insights from the most relevant conversations at the party. These weights are then used to perform a weighted combination of everyones values in order to create a richer, context-aware representation of the word's meaning.\n\nIn our example, \"Right\" in \"Do what is Right\" might learn a stronger \"morality\" value, while in \"Shift to your Right,\" it gains a stronger \"direction\" value.\n\n**The result? Word embeddings that truly reflect their meaning in each sentence, just like people adapting their communication based on the context!**\n\n**Bonus Math (optional):**\n\nThe core calculation behind SDPA involves the attention weight matrix and softmax, expressed as:\n\n$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{{Q.K^T}}{{\\sqrt{d_k}}}\\right) .V$$\n\nwhere:\n\n- $Q$, $K$, $V$ represent the query, key, and value vectors for every word joined as rows of the corresponding matrix.\n> *Note: $K^T$ means that the matrix K was transposed, we do this to align the vectors dimensions apples to apples üçè before the similarity check.*\n\n- **$\\cdot$** denotes the dot product (measuring similarity).\n\n- $\\sqrt{d_k}$ is a scaling factor for stability (so very high scores don't totally drown out lower ones).\n\n- $\\text{softmax}$ distributes weights between 0 and 1 based on attention scores.\n\n- **$\\cdot$** $V$ uses the **weighted attention scores** to do a weighted combination of everyones values to get a new richer embedding.",
   "metadata": {
    "id": "tkYtetKkurT_"
   }
  },
  {
   "cell_type": "markdown",
   "source": "#### **Multi-Head Attention**\nThis is a mechanism in the Self-Attention process where multiple \"heads\" or focus groups are used. Remember our word party? Multi-head attention throws another twist! Imagine multiple groups (heads) at the party, each focusing on different aspects of the conversation. For instance, one head might concentrate on semantic context, while another might focus on emotional context.\n\n**Here are the details:**\n\n* Each group (head) has its own \"attention weight matrix\" showing how relevant other words are to their focus.\n* They analyze independently, like different games happening at once.\n* In the end, they concatenate (Concat in the image above üëÜüëÜ) their insights, creating a richer understanding of each word, like piecing together clues from different groups.\n\n#### **Masked Multi-Head Attention**\nThis is used in the decoder layer to prevent the model from seeing future words. This is achieved by replacing entries above the main diagonal of the attention matrix with `-inf` before performing softmax, a technique known as **Causal Masking**. (Imagine it as putting black tape above the main diagonal of the attention weight matrix).\n\nFor example, in a sentence \"The cat sat on the mat.\", for the word \"sat\", Masked Multi-Head Attention only considers \"The\" and \"cat\", ignoring \"on\", \"the\", and \"mat\".\n\nIn addition to causal masking, a **Padding Mask** is used to prevent the model from attending to `[PAD]` tokens added to equalize the lengths of sequences in a batch. The attention scores of `[PAD]` tokens are set to `-inf`, ensuring these tokens do not affect the final attention output.\n\nIn the case of padding mask, if we have a batch of two sequences: [\"The cat sat down\", \"Good Morning [PAD] [PAD]\"], the model's focus remains solely on the meaningful words in the sequence.\n\nNote: Multi-Head & Masked Multi-Head Attention also have a projection layer (The Linear in the above üëÜüëÜ image). Its job is to project these embeddings, updated from word context, to a more concise form for the Model.\n\nüí°Note: In our implementation below instead of creating multiple heads each with their Networks to get queries (Q), keys (K) and values (V), we are going to cleverly use a big network each for getting Q, K, and V and we will share the output of this networks to all the heads (This bascially does the same thing as with creating multiple heads, this is just a more compute effective way).",
   "metadata": {
    "id": "Tu9uJtkrLYAg"
   }
  },
  {
   "cell_type": "code",
   "source": "import torch.nn.functional as f",
   "metadata": {
    "id": "DBYZszNhkdid"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "class MultiHeadAttention(nn.Module):\n\n    def __init__(self, *, dim_qkv, dim_model, num_heads, causal=False, **kwargs):\n        super().__init__(**kwargs)\n\n        # Ensure dim_qkv (embedding dimension) is divisible by number of heads\n        assert dim_qkv % num_heads == 0, \"DIM_QKV must be divisible by num_heads.\"\n\n        self.num_heads = num_heads\n        self.causal = causal\n\n        # Scaling factor for attention scores\n        self.scale_factor = torch.math.sqrt(dim_qkv//num_heads)\n\n        # The neural networks for queries, keys, and values from the word embeddings\n        self.obtain_queries = nn.Linear(dim_model, dim_qkv, bias=False)\n        self.obtain_keys = nn.Linear(dim_model, dim_qkv, bias=False)\n        self.obtain_values = nn.Linear(dim_model, dim_qkv, bias=False)\n\n        # Final linear projection to get our updated context information back to the same shape as\n        # the input (concise form for the model).\n        self.projection = nn.Linear(dim_qkv, dim_model)\n\n    def forward(self, queries, keys, values, seq_lengths=None):\n\n        # Get our queries, keys, and values from the embeddings\n        queries = self.obtain_queries(queries)\n        keys = self.obtain_keys(keys)\n        values = self.obtain_values(values)\n\n        # Reshape for multi-head attention (will shape our matrix as if we create multiple heads separately).\n        queries = self.parallel_reshape(queries)\n        keys = self.parallel_reshape(keys)\n        values = self.parallel_reshape(values)\n\n        # Calculate attention scores (Remember our attention weight matrix üôÇ)\n        attention = queries @ keys.transpose(-1, -2)\n\n        # Scale scores to prevent saturation (so high scores don't totally drown lower ones)\n        attention = attention / self.scale_factor\n\n        # Apply causal mask (this will be down when we pass causal=true in masked multi-head attention)\n        if self.causal:\n            attention = attention + self.get_causal_mask(queries)\n        else:\n        # Apply padding mask (this will be perform for only reqular multi-head attention)\n        # Note: we don't need padding mask for causal attention because each token can only see previous token\n        # so the padded token can't spoil the embeddings of acutual tokens.\n            attention = torch.masked_fill(attention, self.get_timeseq_mask(seq_lengths), -torch.inf)\n\n        # Calculate attention distribution (softmax)\n        attention = f.softmax(attention, dim=-1)\n\n        # Apply attention distribution to values (Weighted combination of the values to form better contextual embeddings)\n        X = attention @ values\n\n        # Reshape back to original (This is similar to us concatenating the information from multiple heads).\n        X = self.reverse_reshape(X)\n\n        # Final linear projection (Project back our rich embeddings to same shape as the inputs, a concise form for the model)\n        X = self.projection(X)\n\n        return X\n\n    def parallel_reshape(self, tensor):\n        \"\"\"\n        Reshapes the input tensor for efficient dot product computation in multi-head attention.\n\n        This function rearranges the dimensions of the input tensor to facilitate parallel\n        computation across multiple attention heads. It achieves this by:\n\n        1. Reshaping the tensor from (batch_size, seq_len, dim) to (batch_size, seq_len, num_heads, head_dim).\n        2. Permuting the dimensions to (batch_size, num_heads, seq_len, head_dim).\n\n        This new format enables efficient computation of attention scores between queries, keys, and values\n        from different heads in parallel.\n        \"\"\"\n        Batch_size, Seq_len = tensor.shape[0], tensor.shape[1]\n        return tensor.reshape(Batch_size, Seq_len, self.num_heads, -1).permute(0, 2, 1, 3)\n\n    def reverse_reshape(self, tensor):\n        \"\"\"\n        Reshapes the output tensor from multi-head attention back to its original format.\n\n        This function reverses the reshaping performed in `parallel_reshape` to obtain the original\n        tensor format (batch_size, seq_len, dim) used by the rest of the model. It achieves this by:\n\n        1. Permuting the dimensions to (batch_size, seq_len, num_heads, head_dim).\n        2. Reshaping the tensor to (batch_size, seq_len, dim).\n        \"\"\"\n        Batch_size, Seq_len = tensor.shape[0], tensor.shape[2]\n        return tensor.permute(0, 2, 1, 3).reshape(Batch_size, Seq_len, -1)\n\n    def get_timeseq_mask(self, x_lengths):\n        \"\"\"\n        Creates a mask to prevent attention to padded tokens in the sequence.\n\n        This function generates a mask that prevents the attention mechanism from attending to padded\n        tokens in the input sequence. This is crucial because attending to irrelevant padded information\n        can negatively impact the model's performance.\n\n        The mask works by identifying positions in the sequence that are shorter than the corresponding\n        sequence length (valid positions) and setting those positions to `True`. All other positions\n        (padded tokens) are set to `False`.\n        \"\"\"\n        max_seq_len = x_lengths.max().item()  # Get the maximum sequence length in the batch\n\n        # Create a sequence of numbers from 0 to max_seq_len (representing positions in the sequence)\n        ids = torch.arange(0, max_seq_len, device=x_lengths.device)\n\n        # Broadcast the sequence lengths to create a comparison matrix (batch_size, seq_len)\n        # True where a position's id is less than the corresponding sequence length (valid position)\n        mask = ids[None, :] < x_lengths[:, None]\n\n        # Invert the mask to obtain True for positions that should be masked (padded tokens)\n        return ~mask[:, None, None]\n\n\n    def get_causal_mask(self, X):\n        \"\"\"\n        Creates a mask to prevent attention to future tokens in the decoder, enforcing causality.\n\n        This function generates a mask that prevents the decoder from attending to tokens that appear\n        later in the sequence. This is crucial to enforce causality, as the decoder should only use\n        information available up to the current position to predict the next token.\n\n        The mask works by setting all positions above the main diagonal to `True`, effectively blocking\n        the attention mechanism from looking ahead. This mimics the real-world scenario where we cannot\n        predict the future.\n        \"\"\"\n\n        max_seq_len = X.size(2)  # Get sequence length from the input tensor\n        mask = nn.Transformer.generate_square_subsequent_mask(max_seq_len, device=X.device) # generate mask above the main diagonal (like our black tape).\n        return mask\n",
   "metadata": {
    "id": "LBb25L7WVLGS"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### **The Encoder Block**",
   "metadata": {
    "id": "WCt6iIynpVfY"
   }
  },
  {
   "cell_type": "markdown",
   "source": "<img src=\"https://onedrive.live.com/embed?resid=8C3CCBBA832CF1E0%21602&authkey=%21AOjn7M-A8lP-0Is&width=272&height=390\" width=\"272\" height=\"390\" />",
   "metadata": {
    "id": "Z3SmydeilotA"
   }
  },
  {
   "cell_type": "markdown",
   "source": "We crossed out the **Add&Norm** because we are replacing it with **ReZero** in this model (as we will see shortly).\n\nThe first layer shown is a **Multi-Head Attention** layer within the encoder block. This is called a **Self-Attention** layer because the multi-head attention mechanism draws its queries, keys and values only from the encoder input embeddings themselves.\n\nIn other words, the input embeddings are enriched solely based on relationships within themselves, without any external context.\n\nWe also notice the **skip connection** arrow that bypasses this Self-Attention layer, connecting the input directly to the output. This allows signals to propagate easily through the model. With ReZero, we omit the Layer normalization in the Add&Norm, instead doing a weighted addition as follows:\n\n$$\\text{skip} + \\text{re_zero_weight} \\times \\text{output}$$\n\nThere is also a **dropout** layer (not shown in the image) that randomly sets some outputs to zero with a certain probability. This forces the model to utilize as much information as it can get. So the ReZero connection actually looks like:\n\n$$\\text{skip} + \\text{re_zero_weight} \\times \\text{dropout}(\\text{output})$$\n\nNext is the **feed forward network**, whose purpose is to process all the information extracted by the previous layers into a more organized and understandable representation for later stages of the model. It also employs a residual skip connection.\n",
   "metadata": {
    "id": "NYff7uJWpJG0"
   }
  },
  {
   "cell_type": "code",
   "source": "# Note: In transformers, we usually set a size called dim_model. We do this so that the output\n# of all layers, blocks and sub-blocks in the models have the same size since they will almost\n# always be interacting with each other.\n\nclass EncoderBlock(nn.Module):\n    \"\"\"\n    Represents a single encoder block in a Transformer model with ReZero modification.\n    \"\"\"\n\n    def __init__(self, dim_qkv, dim_model, num_heads, dim_ffn, dropout_rate, **kwargs):\n        super().__init__(**kwargs)\n\n        # Multi-Head Self-Attention Layer\n        self.multi_head_attn = MultiHeadAttention(\n            dim_qkv=dim_qkv,  # Dimension of query, key, and value vectors\n            dim_model=dim_model,  # Size of dim_model\n            num_heads=num_heads  # Number of attention heads\n        )\n\n        # Feed-Forward Network\n        self.ffn = nn.Sequential(\n            nn.Linear(dim_model, dim_ffn),  # First linear layer\n            nn.ReLU(),  # ReLU activation for non-linearity (to aid learning complex patterns)\n            nn.Linear(dim_ffn, dim_model)  # Second linear layer\n        )\n\n        # Dropout for regularization (forcing the model to utilize as much information as it can get.)\n        self.dropout = nn.Dropout(dropout_rate)\n\n        # ReZero parameter (learnable weight for weighted addition, typically intialized to zero)\n        self.reZero = nn.Parameter(torch.tensor(0.0))\n\n    def forward(self, X, X_len):\n        \"\"\"\n        Better encodes the input sequence X (in these case our english sentence).\n\n        The X_len is the acutal length of every sentence in the batch (remember we padded the batch),\n        this X_len will be passed to the self-attention layer so it can use it to know the pad tokens to\n        mask during attention.\n        \"\"\"\n\n        # Skip connection for residual addition\n        skip = X\n\n        # Multi-Head Self-Attention with ReZero\n        X = self.multi_head_attn(queries=X, keys=X, values=X, seq_lengths=X_len)\n        X = self.dropout(X)\n        X = skip + self.reZero * X  # ReZero weighted addition\n\n        # Feed-Forward Network with ReZero\n        skip = X\n        X = self.ffn(X)\n        X = self.dropout(X)\n        X = skip + self.reZero * X  # ReZero weighted addition\n\n        return X\n",
   "metadata": {
    "id": "C1lEGl1Pln5U"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "class EncoderLayer(nn.Module):\n    \"\"\"\n    Composes multiple EncoderBlocks to form a deep encoder layer.\n\n    The EncoderLayer holds a collection of chained EncoderBlock modules\n    that are applied sequentially to the input. By chaining multiple blocks,\n    the model can learn increasingly complex and abstract patterns in the\n    input text enabling it to create better representations.\n    \"\"\"\n\n    def __init__(self,\n                 num_encoder_blocks,\n                 dim_qkv,\n                 dim_model,\n                 num_heads,\n                 dim_ffn,\n                 dropout_rate,\n                 **kwargs) -> None:\n\n        super().__init__(**kwargs)\n\n        # Collection of Encoder Blocks\n        self.encoder_blocks = nn.ModuleList([\n            EncoderBlock(\n                dim_qkv,\n                dim_model,\n                num_heads,\n                dim_ffn,\n                dropout_rate\n            ) for _ in range(num_encoder_blocks)\n        ])\n\n    def forward(self, X, X_len):\n        \"\"\"\n        Passes the input through each Encoder Block sequentially.\n        \"\"\"\n        for encoder in self.encoder_blocks:\n            X = encoder(X, X_len)\n\n        return X\n",
   "metadata": {
    "id": "n1458jii2jLR"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### **The Decoder Block**",
   "metadata": {
    "id": "GmSyuk7F3E3B"
   }
  },
  {
   "cell_type": "markdown",
   "source": "<img src=\"https://onedrive.live.com/embed?resid=8C3CCBBA832CF1E0%21603&authkey=%21ALcQ4yPyrJ1GpqM&width=269&height=425\" width=\"269\" height=\"425\" />",
   "metadata": {
    "id": "EVY6GFdA3gSr"
   }
  },
  {
   "cell_type": "markdown",
   "source": "Just like in the encoder block we crossed out the **Add&Norm** because we are replacing it with **ReZero** in this model.\n<br>\n\n**Decoder Block Similarities and Key Differences:**\n\nThe decoder block, like the encoder block, consists of multiple layers that process information sequentially. However, the decoder has two key differences crucial for translating text:\n\n**1. Masked Multi-Head Attention:**\n\n- This layer handles **self-attention** within the decoder's input, like the encoder block.\n\n- **Crucial Difference:** It uses **causal masking** üò∑ to prevent the model from \"cheating\" by peeking at future tokens during training. This aligns with our \"teacher forcing\" technique.\n\n- Imagine it as building a sentence one word at a time, without knowing what the next word will be. This enforces learning based on context and previously generated words.\n\n**2. Cross Attention:**\n\n( The Multi-Head Attention layer highlighted with yellow üü®, also notice the two arrows ‚§¥‚§¥ coming from outside [actually from the encoder block] )\n\n- This layer captures the relationship between the **decoder input** (queries) and the **encoder's enriched representation** (keys and values).\n\n- Think of it as the decoder consulting an \"information sheet\" (encoder's representation) while masked from its future words, helping it understand the context and generate relevant translations.\n\n- The **skip connection** above this layer combines the information from the previous Masked Multi Head Attention and the encoded context (Cross Attention).\n\nüìù Note: In Cross Attention we will also have to pass the actual length of the encoder sentences for masking, this is so we don't try to also get encoded information from the padded [PAD] tokens.",
   "metadata": {
    "id": "jN8lijfV6R03"
   }
  },
  {
   "cell_type": "code",
   "source": "class DecoderBlock(nn.Module):\n    \"\"\"\n    Represents a single decoder block in a Transformer model with ReZero modification.\n    \"\"\"\n\n    def __init__(self, dim_qkv, dim_model, num_heads, dim_ffn, dropout_rate, **kwargs):\n        super().__init__(**kwargs)\n\n        # Masked Multi-Head Attention for self-attention within decoder input\n        self.masked_multi_head_attn = MultiHeadAttention(\n            dim_qkv=dim_qkv,\n            dim_model=dim_model,\n            num_heads=num_heads,\n            causal=True  # Use causal masking to prevent future token peeking\n        )\n\n        # Cross Attention to attend to the encoded representation\n        self.cross_attn = MultiHeadAttention(\n            dim_qkv=dim_qkv,\n            dim_model=dim_model,\n            num_heads=num_heads\n        )\n\n        # Feed-Forward Network just like we saw in encoder block.\n        self.ffn = nn.Sequential(\n            nn.Linear(dim_model, dim_ffn),\n            nn.ReLU(),\n            nn.Linear(dim_ffn, dim_model)\n        )\n\n        # Dropout for regularization (just like in encoder block).\n        self.dropout = nn.Dropout(dropout_rate)\n\n        # ReZero parameter for weighted addition\n        self.reZero = nn.Parameter(torch.tensor(0.0))\n\n    def forward(self, X, enc_outputs, enc_seq_lengths):\n        \"\"\"\n        Processes the decoder input sequence X using attention to the encoded representation.\n\n        Args:\n            X (torch.Tensor): Decoder input sequence of shape (batch_size, seq_len, dim_model).\n\n            enc_outputs (torch.Tensor): Encoded output from the encoder of shape (batch_size, enc_seq_len, dim_model).\n\n            enc_seq_lengths (torch.Tensor): Sequence lengths for the encoder of shape (batch_size,).\n\n        Returns:\n            torch.Tensor: Updated decoder output of shape (batch_size, seq_len, dim_model).\n        \"\"\"\n\n        # Skip connection for residual addition\n        skip = X\n\n        # Masked Multi-Head Attention (Self-Attention)\n        # - Prevents peeking at future words during training\n        # - Focuses on context within the decoder's input\n        X = self.masked_multi_head_attn(queries=X, keys=X, values=X)\n        X = self.dropout(X)\n        X = skip + self.reZero * X  # ReZero weighted addition\n\n        # Skip connection for residual addition\n        skip = X\n\n        # Cross Attention\n        # - Attends to the encoded representation for context\n        # - Combines information from decoder and encoder\n        # - Also pass the encoder sequence lengths so we don't try to get encoded information from [PAD] tokens.\n        X = self.cross_attn(queries=X, keys=enc_outputs, values=enc_outputs, seq_lengths=enc_seq_lengths)\n        X = self.dropout(X)\n        X = skip + self.reZero * X  # ReZero weighted addition\n\n        # Skip connection for residual addition\n        skip = X\n\n        # Feed-Forward Network (just like we have seen before).\n        X = self.ffn(X)\n        X = self.dropout(X)\n        X = skip + self.reZero * X  # ReZero weighted addition\n\n        return X\n",
   "metadata": {
    "id": "X_f-NfsLBQXd"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "class DecoderLayer(nn.Module):\n    \"\"\"\n    Composes multiple DecoderBlocks to form a deeper decoder layer.\n    \"\"\"\n\n    def __init__(self, num_decoder_blocks, dim_qkv, dim_model, num_heads, dim_ffn, dropout_rate, **kwargs):\n        super().__init__(**kwargs)\n\n        # Collection of Decoder Blocks\n        self.decoder_blocks = nn.ModuleList([\n            DecoderBlock(\n                dim_qkv,\n                dim_model,\n                num_heads,\n                dim_ffn,\n                dropout_rate\n            ) for _ in range(num_decoder_blocks)\n        ])\n\n    def forward(self, X, enc_out, enc_seq_lengths):\n        \"\"\"\n        Passes the input through each Decoder Block sequentially.\n        \"\"\"\n        for decoder in self.decoder_blocks:\n            X = decoder(X, enc_out, enc_seq_lengths)\n\n        return X",
   "metadata": {
    "id": "Pddq1wWgEwaf"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Wow üòÅ, we have come a long way but before we start putting the pieces together to build our Transformer let's define some Hyper-Parameters (This means parameters that determines the size of the components in our architecture) for our model.",
   "metadata": {
    "id": "plChtv-4GDJk"
   }
  },
  {
   "cell_type": "code",
   "source": "# Model Hyper-Parameters\n\n# To ensure the output of all blocks, sub-blocks in the model have the same size as they will be interacting.\nDIM_MODEL = 768\n\n# The Size our feed-forward networks first expands its inputs `to` before performing nn.ReLU (non-linearity to capture complex patterns)\n# and later projecting back to the inputs size.\nDIM_FFN = 784\n\n# The Size of the queries, keys and values in the attention blocks.\n# Note: This will be shared equally among all the heads, so it must be divisible by the number of heads we choose.\nDIM_QKV = 512\n\n# The probability of which to turn off (Zero) outputs. 0.2 means 20 %\nDROPOUT_RATE = 0.2\n\n# The vocabulary size.\nVOCAB_SIZE = tokenizer.vocab_size",
   "metadata": {
    "id": "jW16ub11HtAm"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### **Putting the Pieces Together: Transformer**",
   "metadata": {
    "id": "3ac31ZqfKSpK"
   }
  },
  {
   "cell_type": "markdown",
   "source": "<img src=\"https://onedrive.live.com/embed?resid=8C3CCBBA832CF1E0%21604&authkey=%21AHp5UHZoaJ4wLnA&width=724&height=832\" width=\"724\" height=\"832\" />",
   "metadata": {
    "id": "KXjiyjtSKbC2"
   }
  },
  {
   "cell_type": "markdown",
   "source": "Let's talk about the newly added portion (the section on top highlighted in green). This section explains how we use the information from the decoder layer (which also incorporates the information from the encoder layer) to predict the next translated word. To do this, we have a linear layer that takes this information and assigns prediction scores (called logits) to every word in our vocabulary.\n\nThe output size of this layer should match the vocabulary size, so that each word has a corresponding score. The higher the score, the more confident the model is that that word is the next one.\n\nWe also have a softmax layer that transforms these scores into probabilities between 0 and 1. These probabilities are then outputted by the model.\n\nüìù Note: We asterisk (*) the softmax function because the specific error function we are going to use in PyTorch (as you will see below) will automatically apply it for us, so we just need to return the logits.\n\nNow we need an error function, which is a function that our model tries to minimize. By minimizing the error, the model learns how to perform our task (language translation in this case).\n\nWe are going to use CrossEntropyLoss as our error function. This function takes the logits representing the model's prediction of the next translated word and compares it with the actual next word. It then gives an error value based on how far off the model's prediction is from the actual word. The model can then adjust itself accordingly (thereby learning).\n\n**Bonus Technical Detail**: CrossEntropyLoss is a negative log loss function. How does this work? Well, we take the logits and apply a softmax function on them to convert them into probabilities between 0 and 1.\n\nWe want the model to predict 1 for the actual next word (meaning the other probabilities will be 0), so how do we get an error value out of these? Well, we apply a logarithm function on the model's prediction of the actual word. If the model correctly predicts 1, then log(1) is 0, so the error value is 0.\n\nThis tells the model that it is on the right track and there is no error. But if the model's prediction is less than 1, then the logarithm function starts to approach $-\\infty$, which means the error value becomes very small.\n\nThis is a problem because we want to minimize the error function, and the way it is shaped now, the model would learn to predict 0 for the right word in order to reduce the error.\n\nSo here is where the trick comes in: we multiply the logarithm function by `-1`, so that now for predictions less than 1, the error value starts to approach $\\infty$ instead. This forces the model to learn to predict 1 for the right word.\n\nThis is why it is called negative log loss: $-log(models\\ prediction \\ for\\ actual\\ next\\ word)$.\n\nNow because we are predicting the next word multiple times in a sample, and for every sample in the batch, we calculate all the error values and average them, so that we have one error value to minimize.\n",
   "metadata": {
    "id": "_64Go-N-Q9AU"
   }
  },
  {
   "cell_type": "code",
   "source": "# Import neccesary to calculate the accuracy.\nfrom torchmetrics import Accuracy",
   "metadata": {
    "id": "cJuJn_8Ge84K"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# We will be using a Pytorch Lightning Module, Lightning allows building easier training process for Models\n# it helps get rid of boilerplate code use in vanilla Pytorch.\n\nclass Transformer(L.LightningModule):\n    \"\"\"\n    This class defines the Transformer model for the translation task.\n\n    Attributes:\n        pos_embed_layer (LearnedPositionalEncoding): Adds positional encodings to the input sequences.\n        embedding_layer (nn.Embedding): Embeds the input tokens into vector representations.\n        encoder_layer (EncoderLayer): Encoder layer of the model with stacked Encoder Blocks.\n        decoder_layer (DecoderLayer): Decoder layer of the model with stacked Decoder Blocks.\n        output (nn.Linear): Final layer mapping decoder outputs to vocabulary size for prediction.\n        loss_fn (nn.CrossEntropyLoss): Loss function with padding token ignored.\n        accuracy (Accuracy): Metric for calculating accuracy of model translation with padding token ignored.\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\n        # Positional Encoding\n        self.pos_embed_layer = LearnedPositionalEncoding(\n            pos_embed_layer=pos_embedding_layer  # Inject our pre-trained positional embeddings layer.\n        )\n\n        # Inject our pre-trained token embedding layer.\n        self.embedding_layer = embedding_layer\n\n        # Encoder\n        self.encoder_layer = EncoderLayer(\n            num_encoder_blocks=4,  # Hyperparameter: Number of encoder blocks\n            dim_qkv=DIM_QKV,  # Hyperparameter: Dimension of query, key, and value vectors\n            dim_model=DIM_MODEL,  # Hyperparameter: Ensure similar output sizes across the model\n            num_heads=8,  # Hyperparameter: Number of attention heads in each block\n            dim_ffn=DIM_FFN,  # Hyperparameter: Dimension of feed-forward network\n            dropout_rate=DROPOUT_RATE  # Hyperparameter: Dropout rate for regularization\n        )\n\n        # Decoder\n        self.decoder_layer = DecoderLayer(\n            num_decoder_blocks=4,  # Hyperparameter: Number of decoder blocks\n            dim_qkv=DIM_QKV,  # Hyperparameter: Dimension of query, key, and value vectors\n            dim_model=DIM_MODEL,  # Hyperparameter: Ensure similar output sizes across the model\n            num_heads=8,  # Hyperparameter: Number of attention heads in each block\n            dim_ffn=DIM_FFN,  # Hyperparameter: Dimension of feed-forward network\n            dropout_rate=DROPOUT_RATE  # Hyperparameter: Dropout rate for regularization\n        )\n\n        # Output Layer\n        self.output = nn.Linear(DIM_MODEL, VOCAB_SIZE)  # Project to vocabulary size for prediction\n\n        # Loss Function and Metrics\n        self.loss_fn = nn.CrossEntropyLoss(ignore_index=PAD_IDX)  # Ignore padding tokens\n        self.accuracy = Accuracy(\n            task='multiclass',\n            num_classes=VOCAB_SIZE,\n            ignore_index=PAD_IDX # Ignore padding tokens\n        )\n\n    def forward(self, X_enc, X_enc_len, X_dec):\n        \"\"\"\n        Forward pass through the Transformer model.\n\n        Args:\n            X_enc (torch.Tensor): Input sequence for the encoder.\n            X_enc_len (torch.Tensor): Sequence lengths for the encoder.\n            X_dec (torch.Tensor): Input sequence for the decoder.\n\n        Returns:\n            torch.Tensor: Logits of the predicted tokens.\n        \"\"\"\n\n        # Get embeddings\n        X_enc = self.embedding_layer(X_enc)  # Embed encoder input\n        X_dec = self.embedding_layer(X_dec)  # Embed decoder input\n\n        # Add positional encodings\n        X_enc = self.pos_embed_layer(X_enc)  # Add positional information to encoder\n        X_dec = self.pos_embed_layer(X_dec)  # Add positional information to decoder\n\n        # Encoder outputs\n        X_enc = self.encoder_layer(X_enc, X_enc_len)  # Pass through encoder layers\n\n        # Decoder outputs (which also incorporates the information from the encoder layer)\n        X_dec = self.decoder_layer(X_dec, enc_out=X_enc, enc_seq_lengths=X_enc_len)\n\n        # Final output layer (maps to vocabulary size) to get the logits of the predictions.\n        logits = self.output(X_dec)\n\n        return logits\n\n    def training_step(self, batch, batch_idx):\n        \"\"\"\n        Performs a single training step.\n\n        Args:\n            batch (dict): Batch of data containing encoded and decoded sequences.\n            batch_idx (int): Index of the current batch.\n\n        Returns:\n            loss: Calculated loss value for the current batch.\n        \"\"\"\n\n        return self._common_step(batch, 'train')\n\n    def validation_step(self, batch, batch_idx):\n        \"\"\"\n        Performs a single validation step.\n\n        Args:\n            batch (dict): Batch of data containing encoded and decoded sequences.\n            batch_idx (int): Index of the current batch.\n\n        Returns:\n            loss: Calculated loss value for the current batch.\n        \"\"\"\n\n        return self._common_step(batch, 'val')\n\n    def _common_step(self, batch, prefix):\n        \"\"\"\n        Shared logic for training and validation steps.\n\n        Args:\n            batch (dict): Batch of data containing encoded and decoded sequences.\n            prefix (str): Prefix for logging metrics ('train' or 'val').\n\n        Returns:\n            loss: Calculated loss value for the current batch.\n        \"\"\"\n\n        # Unpack batch data\n        X_enc, X_enc_len, X_dec, Y_dec = batch\n\n        # Run forward pass (asking the model to make prediction).\n        logits = self.forward(X_enc, X_enc_len, X_dec)\n\n        # Calculate loss (ignoring padding tokens)\n        # Note: By permute(...) we are reshaping the logits to the form the function accepts as stated in Pytorch Documentation Online.\n        loss = self.loss_fn(logits.permute(0, 2, 1), Y_dec)\n\n        # Calculate and log accuracy to our training progress bar so we can see if\n        # the model is improving while training (ignoring padding tokens).\n        self.log_dict({\n            f'{prefix} acc': self.accuracy(logits.permute(0,2,1), Y_dec),\n            f'{prefix} loss': loss\n        },\n        prog_bar=True) # show metrics on the progress bar.\n\n        return loss\n\n    def configure_optimizers(self):\n        \"\"\"\n        Configures the optimizer used for training the model (view this as a coach that tells the model to improve its components,\n        thats why we pass to it the model parameters so it can guide the tuning of them)\n\n        Returns:\n            torch.optim.Optimizer: The chosen optimizer instance.\n        \"\"\"\n\n        optimizer = torch.optim.Adam(self.parameters(), lr=5e-4)  # Example using Adam with learning rate 5e-4\n        return optimizer\n\n",
   "metadata": {
    "id": "jlHzOAjRKacj"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### **Architecture Summary**\nWith the aid of torchinfo we are going to print a summary of our model to see details like number of trainable parameters, and non-trainable parameters (remember we freezed the parameters in our pre-trained word and positional embeddings layers).",
   "metadata": {
    "id": "BwLMfA0MlOEj"
   }
  },
  {
   "cell_type": "code",
   "source": "!pip install torchinfo",
   "metadata": {
    "id": "pHzTUtAJjm-h"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "from torchinfo import summary\n\nbatch_size = 24 # will emulate passing a batch size of 24 through the model.\nsummary(\n    Transformer(),\n    input_data = [\n        torch.randint(low=2, high=250, size=(batch_size, TOKEN_LIMIT)), # emulate encoder inputs\n        torch.full([batch_size], TOKEN_LIMIT), # emulate lengths of encoder inputs\n        torch.randint(low=2, high=250, size=(batch_size, TOKEN_LIMIT)) # emulate decoder inputs\n    ],\n    device='cpu'\n)",
   "metadata": {
    "id": "RP8NZNFBjUiZ",
    "outputId": "e5d6a7de-abb7-44e7-bae1-afc6548b3581"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### **Training Our Model**",
   "metadata": {
    "id": "kAVJM5a8nA14"
   }
  },
  {
   "cell_type": "code",
   "source": "# Since we are going to be training our model for several epochs (An epoch is a full iteration through our dataset [of 2_007_723 sentence pairs]),\n# we want to save the model at intervals so if there is any unforseen occurance, we don't just have to start form the beginning.\nfrom lightning.pytorch.callbacks import ModelCheckpoint\n\ncheckpoint_callback = ModelCheckpoint(\n    dirpath='latest_ckpt/',\n    filename='en_fr_model',\n    every_n_train_steps=200, # After how many batches should we save the model.\n    save_last=True, # Save the current model at that point.\n    # In addition to saving the current model we also want to save the best model so\n    # far so incase the model suddenly starts back-tracking, we can revert to the best model so far.\n    save_top_k=1\n)",
   "metadata": {
    "id": "TdpWQSzZnFdk"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# We use a trainer provided by Pytorch Lightning to train the model\n# (Note: It will automatically use a GPU, TPU, IPU or HPU if any is available).\ntrainer = L.Trainer(\n    max_epochs=5, # Train for 5 epochs.\n    callbacks=[checkpoint_callback] # Register our checkpoint_callback to tell the trainer to save the checkpoints.\n)",
   "metadata": {
    "id": "cvSsBFFQoXVE",
    "outputId": "6f8afcc8-0df6-49ef-d326-f240fec8bccb"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# create an instance of the transformer to train.\ntransformer_model = Transformer()",
   "metadata": {
    "id": "FyY0btAmr3Ha"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# pass our instance to the trainer for training\ntrainer.fit(\n    transformer_model, # our transformer instance\n    translation_datamodule # the data module we previously created, will be used for training the model.\n)",
   "metadata": {
    "id": "2jEblmMssHDw",
    "outputId": "19f20302-4f8f-42bf-9bd7-180ab9f1e93e"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### **Testing Our Model**\nI already trained a Model though i only trained for half of the first epoch it reached up to train acc = 0.62 a 62% percent accuracy in training data next word prediction. I will use that model.\n\nNote: This model is not added to the notebooks due to some issues i had while creating this notebooks, but you can train your own model (should take you about 3 hours to train on half of the first epoch with Nvidia T4 or P100 gpu that is on colab or kaggle).",
   "metadata": {
    "id": "77jOG0Htu5Fo"
   }
  },
  {
   "cell_type": "code",
   "source": "# This particular line was how i loaded the model on my machine, wont work on yours\n# as i stated in the markdown above üëÜüëÜ\nmy_trained_model_path = '/content/drive/MyDrive/Public Educational Notebooks/Lang_Translate/Resources/model.ckpt'\n\nloaded_model = Transformer.load_from_checkpoint(my_trained_model_path)",
   "metadata": {
    "id": "tQt5vEnzsanK"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def naive_greedy_decoding(en_sentence, transformer_model, tokenizer, max_output_len=TOKEN_LIMIT):\n    \"\"\"\n    Performs basic greedy decoding on the provided English sentence using the given Transformer model.\n\n    This method serves for educational purposes and demonstrates a simple greedy decoding approach.\n    For practical applications, more efficient methods like KV caching are used.\n\n    Args:\n        en_sentence (str): The English sentence to translate.\n        transformer_model (nn.Module): The trained Transformer model.\n        tokenizer (transformers.PreTrainedTokenizer): The tokenizer used for text and vocabulary handling.\n        max_output_len (int, optional): Maximum length of the decoded French sentence. Defaults to 350 (Our Models TOKEN_LIMIT).\n\n    Returns:\n        str: The decoded French sentence generated by the model.\n\n    Raises:\n        Exception: If the input sentence is empty.\n    \"\"\"\n\n    # **Evaluation mode and GPU usage:**\n    transformer_model = transformer_model.eval().cuda()  # Switch to evaluation mode and move to GPU\n\n    # **Input validation:**\n    assert isinstance(en_sentence, str), \"The english sentence should be a string\"\n    en_sentence = en_sentence.strip()\n\n    if en_sentence == \"\":\n        raise Exception('Text should not be empty')\n\n    en_sentence = clean_text(en_sentence)  # Apply any necessary text cleaning\n\n    # **Tokenization and sequence lengths:**\n    en_sen = tokenizer.encode(en_sentence, return_tensors='pt')  # Tokenize English sentence\n    en_sen_len = torch.tensor([en_sen.shape[1]])  # Calculate input sequence length\n\n    # **Initialize decoded sentence and loop:**\n    fr_decoded = [tokenizer.cls_token_id]  # Start with [CLS] token representing our Start Of Sentence\n    for _ in range(max_output_len):\n\n        # **Disable gradient calculation for efficiency:**\n        with torch.no_grad():\n            # **Forward pass through the Transformer:**\n            log_proba = transformer_model(\n                en_sen.cuda(),  # Input English sentence on GPU\n                en_sen_len.cuda(),  # Input English sentence length on GPU\n                torch.tensor([fr_decoded]).cuda()  # Current decoded sentence on GPU\n            )  # Shape: [1, len(fr_decoded), vocab_size]\n\n            # **Greedy decoding: choose word with highest probability**\n            next_word_id = torch.argmax(log_proba, dim=-1)[0][-1]\n            fr_decoded.append(next_word_id)  # Add predicted word to decoded sentence\n\n            # **Early stopping if [SEP] (our end-of-sentence) token is predicted**\n            if next_word_id == tokenizer.sep_token_id:\n                break\n\n    # **Decode tokens back to human-readable text:**\n    fr_text = tokenizer.decode(\n        fr_decoded,\n        clean_up_tokenization_spaces=True  # Remove extra spaces around punctuation\n    ).replace(' ##', '')  # Join subwords back into complete words\n\n    return fr_text",
   "metadata": {
    "id": "YIwlq8cv0qke"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "text = \"I will like to address the court tomorrow, after much delibration on the matter\"\n\nnaive_greedy_decoding(\n    text,\n    loaded_model,\n    tokenizer\n    )",
   "metadata": {
    "id": "zH0JCttW1OSs",
    "outputId": "a640dab5-9247-4d68-da83-f25e6e3e9469"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "<img src=\"https://onedrive.live.com/embed?resid=8C3CCBBA832CF1E0%21605&authkey=%21AN-UxmT6yETwyKs&width=975&height=298\" width=\"975\" height=\"298\" />",
   "metadata": {
    "id": "FbsT5hLdA7aO"
   }
  },
  {
   "cell_type": "markdown",
   "source": "### **Beam Search**\nMachine translation models generate output sequences like translated sentences. A common challenge is how to select the best words at each step, given the previous words and the input sequence.\n\nOne simple method is **greedy decoding**, which predicts the most likely word at each step, building the sentence word by word. However, this can lead to suboptimal results as the model might get stuck in local optima, missing better translations overall.\n\nA more sophisticated method is **beam search**, which is a decoding algorithm that can generate sequences of words from a probability distribution over the vocabulary. It is often used in natural language processing tasks such as machine translation, text summarization, and image captioning, where the output is a sequence of words.\n\nUnlike greedy decoding, which only selects the most likely word at each step, beam search keeps track of a fixed number of candidates (called the beam size) and expands them until the end of the sequence or a stop token is reached. This allows beam search to explore more possible sequences and find a better solution than greedy decoding.\n\nThe main steps of beam search are:\n\n1. **Maintain multiple candidate translations (beams).** At each step, instead of just the single best word, we consider the top `k` most likely words for **each existing beam**.\n\n2. **Expand each beam with the chosen word.** This effectively creates `k` new beams for each existing one, exploring different translation paths in parallel.\n\n3. **Evaluate and score new beams.** We consider both the probability of the current word and the overall translation's likelihood based on previous words.\n\n4. **Keep the top `k` beams.** We prune unlikely translations, focusing on the most promising candidates.\n\n5. **Repeat steps 2-4 until reaching a maximum length or end-of-sentence marker.**\n\nBy considering multiple possibilities simultaneously, beam search **increases the chance of finding better translations** compared to greedy decoding. However, it comes with increased computational cost due to handling more candidate sequences.\n\nBeam search is needed because it can improve the quality and diversity of the generated sequences. Beam search can avoid some of the problems of greedy decoding, such as repeating words or generating short and incomplete sentences. Beam search can also generate multiple sequences with different probabilities, which can be useful for tasks that require multiple outputs or evaluation metrics.",
   "metadata": {
    "id": "WLi23pbxAHq5"
   }
  },
  {
   "cell_type": "code",
   "source": "import numpy as np\n\ndef naive_beam_search(en_sentence, transformer_model, tokenizer, beam_width, max_output_len=250):\n    \"\"\"\n    Performs basic beam search decoding on the provided English sentence using the given Transformer model.\n\n    This method serves for educational purposes and demonstrates a simple beam search decoding approach.\n    For practical applications, more advanced and optimized beam search implementations are available.\n\n    Args:\n        en_sentence (str): The English sentence to translate.\n        transformer_model (nn.Module): The trained Transformer model.\n        tokenizer (transformers.PreTrainedTokenizer): The tokenizer used for text and vocabulary handling.\n        beam_width (int): The number of beams to keep during decoding.\n        max_output_len (int, optional): Maximum length of the decoded French sentence. Defaults to 250.\n\n    Returns:\n        list: A list of the top beam translations decoded by the model.\n    \"\"\"\n\n    # Set BOS and EOS token IDs for consistency\n    tokenizer.bos_token_id = tokenizer.cls_token_id\n    tokenizer.eos_token_id = tokenizer.sep_token_id\n\n    # Switch model to evaluation mode and GPU for efficiency\n    transformer_model = transformer_model.eval().cuda()\n\n    # Input validation\n    assert isinstance(en_sentence, str), \"The english sentence should be a string\"\n    en_sentence = en_sentence.strip()\n\n    if not en_sentence:\n        raise ValueError('Text should not be empty')\n    en_sentence = clean_text(en_sentence)  # Apply any necessary text cleaning\n\n    # Tokenize English sentence and calculate sequence length\n    en_sen = tokenizer.encode(en_sentence, return_tensors='pt')\n    en_sen_len = torch.tensor([en_sen.shape[1]])\n\n    # Initialize beam search data structures\n    with torch.no_grad():\n        # Get logits for first word from all beams\n        logits = transformer_model(\n            en_sen.cuda(),\n            en_sen_len.cuda(),\n            torch.tensor([[tokenizer.bos_token_id]]).cuda()\n            )[0, 0]\n\n        logits = torch.log(f.softmax(logits, dim=-1))  # Apply log-softmax\n\n        # Select top k logits and indices for each beam\n        top_k_logits, top_k_ind = torch.topk(logits, k=beam_width)\n\n        # Initialize beam translations, lengths, and EOS flags\n        fr_alt_translations = np.empty(beam_width, dtype=object)\n        fr_alt_translations[:] = [[tokenizer.bos_token_id, top_k_ind[i].item()] for i in range(beam_width)]\n        alt_sen_length = torch.tensor([2] * beam_width).cuda()  # Start with length 2 (BOS + first word)\n        non_eos_ind = torch.tensor([True] * beam_width).cuda()  # All beams active initially\n\n    # Main beam search loop\n    for _ in range(max_output_len):\n\n        # Initialize storage for next word logits\n        store_logits = torch.full((beam_width, tokenizer.vocab_size), -torch.inf).cuda()\n\n        # Set logits for beams that already reached EOS\n        store_logits[~non_eos_ind, tokenizer.eos_token_id] = top_k_logits[~non_eos_ind]\n\n        with torch.no_grad():\n            # Get logits for next word predictions for active beams\n            store_logits[non_eos_ind] = transformer_model(\n                en_sen.repeat(sum(non_eos_ind), 1).cuda(),\n                en_sen_len.repeat(sum(non_eos_ind)).cuda(),\n                torch.tensor(\n                    fr_alt_translations[non_eos_ind.cpu()].tolist()\n                    ).cuda()\n            )[:, -1]  # Get logits for last word in each sentence\n\n        # Mask unknown token with -inf logits\n        # store_logits[:, tokenizer.unk_token_id] = -torch.inf\n\n        # Apply log-softmax and add previous beam logits for active beams\n        store_logits[non_eos_ind] = torch.log(f.softmax(store_logits[non_eos_ind], dim=-1))\n        store_logits[non_eos_ind] += top_k_logits[non_eos_ind].view(-1, 1)\n\n        # Sentence length normalization:\n        len_norm_factor = 1/alt_sen_length.view(-1, 1) ** 0.7  # Calculate length normalization factor\n        store_logits *= len_norm_factor  # Apply normalization to logits\n\n        # Select top k next word predictions and indices:\n        top_k_logits, top_k_ind = torch.topk(store_logits.view(-1), k=beam_width)  # Reshape and get top k\n\n        # Extract next word IDs and beam indices:\n        next_token_id = top_k_ind % tokenizer.vocab_size  # Extract word IDs from indices\n        beam_ind = top_k_ind // tokenizer.vocab_size  # Extract beam indices\n\n        # Identify beams that haven't reached EOS:\n        non_eos_ind = next_token_id != tokenizer.eos_token_id  # Check for EOS token in predictions\n\n        # Select active beams (not reached EOS) for further processing:\n        fr_alt_translations = fr_alt_translations[beam_ind.cpu()]  # Keep active translations\n        alt_sen_length = alt_sen_length[beam_ind.cpu()]  # Keep active sentence lengths\n\n        # Update active beams with new word and increment length:\n        for idx, next_id in zip(torch.nonzero(non_eos_ind).flatten(), next_token_id[non_eos_ind]):\n            fr_alt_translations[idx] = fr_alt_translations[idx] + [next_id.item()]  # Add next word\n            alt_sen_length[idx] += 1  # Increase sentence length\n\n        # Early stopping if all beams have reached EOS:\n        if sum(non_eos_ind) == 0:\n            break  # No active beams left, stop decoding\n\n    # Decode final translations and return results:\n    for i, translation in enumerate(fr_alt_translations):\n        fr_alt_translations[i] = tokenizer.decode(\n            translation,\n            clean_up_tokenization_spaces=True,\n            skip_special_tokens=True\n        ).replace(' ##', '')  # Decode, remove spaces, combine subwords\n\n    return fr_alt_translations, top_k_logits  # Return decoded translations and final logits\n",
   "metadata": {
    "id": "yPQFXf6u6IzF"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "text = '''\n    While we disagree on specifics, we have a duty to serve the citizens of this country\n    to the best of our abilities.\n'''\n\nnaive_beam_search(\n    text,\n    loaded_model,\n    tokenizer,\n    3)",
   "metadata": {
    "id": "N1XWSmqR7v6X",
    "outputId": "af636a88-d47b-41b9-cf28-e817155764c8"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "<img src=\"https://onedrive.live.com/embed?resid=8C3CCBBA832CF1E0%21606&authkey=%21AMbl-Me9ZEZ-WvI&width=990&height=394\" width=\"990\" height=\"394\" />",
   "metadata": {
    "id": "5OF908LbMVHs"
   }
  },
  {
   "cell_type": "markdown",
   "source": "üìù Note: Although Our Model performs fairly well for words you might normally hear at congress, it stuggles with normal texts üëáüëá",
   "metadata": {
    "id": "YCTyFzNcTGC2"
   }
  },
  {
   "cell_type": "code",
   "source": "text = '''\n    Wow!, What a beautiful day for hunting deers.\n'''\n\nnaive_beam_search(\n    text,\n    loaded_model,\n    tokenizer,\n    3)",
   "metadata": {
    "id": "-f-r5driN-4K",
    "outputId": "c0c024f4-c0ec-43dd-c10a-ec33e67d849e"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "<img src=\"https://onedrive.live.com/embed?resid=8C3CCBBA832CF1E0%21607&authkey=%21APkk4TZjfybDQPw&width=979&height=297\" width=\"979\" height=\"297\" />",
   "metadata": {
    "id": "qXvYML9qSw6a"
   }
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "id": "dJO5bHB_EpAG"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
